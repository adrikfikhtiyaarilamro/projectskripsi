{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6088c6f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-07T00:21:42.855844Z",
     "iopub.status.busy": "2026-01-07T00:21:42.855620Z",
     "iopub.status.idle": "2026-01-07T00:21:47.957778Z",
     "shell.execute_reply": "2026-01-07T00:21:47.956960Z"
    },
    "papermill": {
     "duration": 5.107363,
     "end_time": "2026-01-07T00:21:47.959058",
     "exception": false,
     "start_time": "2026-01-07T00:21:42.851695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random, math, time, json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "SR_LIST = [16000]  \n",
    "SR_LIST = sorted(set(int(s) for s in SR_LIST))  \n",
    "SR = SR_LIST[-1] \n",
    "\n",
    "NOISE_COLOR = \"white\"  \n",
    "N_MFCC = 30           \n",
    "N_MELS = 40           \n",
    "\n",
    "def mfcc_params_for_sr(sr: int):\n",
    "    sr = int(sr)\n",
    "    if sr == 16000:\n",
    "        return {\"n_fft\": 512, \"win_length\": 400, \"hop_length\": 160}\n",
    "    if sr == 8000:\n",
    "        return {\"n_fft\": 256, \"win_length\": 200, \"hop_length\": 80}\n",
    "    win = int(sr * 0.025)\n",
    "    hop = int(sr * 0.010)\n",
    "    def _next_pow2(x):\n",
    "        p = 1\n",
    "        while p < x:\n",
    "            p <<= 1\n",
    "        return p\n",
    "    n_fft = _next_pow2(win)\n",
    "    return {\"n_fft\": n_fft, \"win_length\": win, \"hop_length\": hop}\n",
    "\n",
    "if \"COMMIT_MODE\" not in globals():\n",
    "    COMMIT_MODE = False\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42, deterministic: bool = True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = bool(deterministic)\n",
    "    torch.backends.cudnn.benchmark = not bool(deterministic)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "PIN_MEMORY = True if USE_CUDA else False\n",
    "NON_BLOCK = True if USE_CUDA else False\n",
    "\n",
    "if not COMMIT_MODE:\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "ROOT_DIR = \"/kaggle/input/speech-commands\"\n",
    "NOISE_DIR = \"/kaggle/input/speech-commands/_background_noise_\"\n",
    "WHITE_NOISE_FILE = \"white_noise.wav\"\n",
    "PINK_NOISE_FILE  = \"pink_noise.wav\"\n",
    "\n",
    "SELECTED_CLASSES = [\"down\", \"left\", \"right\", \"up\"]\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(SELECTED_CLASSES)}\n",
    "\n",
    "MAX_LENGTH = SR  \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "N_SPLITS = 5\n",
    "SEEDS = [36, 38, 42]\n",
    "\n",
    "NOISE_PROB = 0.30\n",
    "SNR_RANGE = (5, 20)\n",
    "\n",
    "NUM_WORKERS = max(2, (os.cpu_count() or 2) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72920733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:21:47.965739Z",
     "iopub.status.busy": "2026-01-07T00:21:47.965048Z",
     "iopub.status.idle": "2026-01-07T00:21:47.988610Z",
     "shell.execute_reply": "2026-01-07T00:21:47.987869Z"
    },
    "papermill": {
     "duration": 0.028026,
     "end_time": "2026-01-07T00:21:47.989829",
     "exception": false,
     "start_time": "2026-01-07T00:21:47.961803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resolve_noise_path(noise_dir: str, filename: str):\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = os.path.join(noise_dir, filename)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "WHITE_PATH = resolve_noise_path(NOISE_DIR, WHITE_NOISE_FILE)\n",
    "PINK_PATH  = resolve_noise_path(NOISE_DIR, PINK_NOISE_FILE)\n",
    "\n",
    "if str(NOISE_COLOR).lower() == \"pink\":\n",
    "    ACTIVE_NOISE_PATH = PINK_PATH\n",
    "    ACTIVE_NOISE_NAME = \"pink\"\n",
    "else:\n",
    "    ACTIVE_NOISE_PATH = WHITE_PATH\n",
    "    ACTIVE_NOISE_NAME = \"white\"\n",
    "\n",
    "if not COMMIT_MODE:\n",
    "    print(f\"Using noise: {ACTIVE_NOISE_NAME} -> {ACTIVE_NOISE_PATH}\")\n",
    "\n",
    "def add_specific_noise(\n",
    "    waveform: torch.Tensor,\n",
    "    noise_path: str,\n",
    "    snr_db: float,\n",
    "    target_sr: int = SR\n",
    "):\n",
    "    if (noise_path is None) or (not os.path.exists(noise_path)):\n",
    "        return waveform\n",
    "\n",
    "    noise, sr = torchaudio.load(noise_path)   \n",
    "    if noise.dim() == 1:\n",
    "        noise = noise.unsqueeze(0)\n",
    "    if noise.shape[0] > 1:\n",
    "        noise = noise.mean(dim=0, keepdim=True)  \n",
    "\n",
    "    if sr != target_sr:\n",
    "        noise = torchaudio.functional.resample(noise, sr, target_sr)\n",
    "\n",
    "    T = waveform.shape[1]\n",
    "    if noise.shape[1] < T:\n",
    "        repeat = math.ceil(T / noise.shape[1])\n",
    "        noise = noise.repeat(1, repeat)\n",
    "    noise = noise[:, :T]\n",
    "\n",
    "    noise = noise.to(waveform.device, dtype=waveform.dtype)\n",
    "\n",
    "    sig_power = waveform.pow(2).mean().clamp(min=1e-12)\n",
    "    noise_power = noise.pow(2).mean().clamp(min=1e-12)\n",
    "    snr_linear = 10.0 ** (snr_db / 10.0)\n",
    "    scale = torch.sqrt(sig_power / (snr_linear * noise_power))\n",
    "\n",
    "    noisy = waveform + scale * noise\n",
    "    return torch.clamp(noisy, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14b5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:21:47.995914Z",
     "iopub.status.busy": "2026-01-07T00:21:47.995378Z",
     "iopub.status.idle": "2026-01-07T00:21:48.004417Z",
     "shell.execute_reply": "2026-01-07T00:21:48.003883Z"
    },
    "papermill": {
     "duration": 0.013188,
     "end_time": "2026-01-07T00:21:48.005425",
     "exception": false,
     "start_time": "2026-01-07T00:21:47.992237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VoiceCommandDatasetWithNoise(Dataset):\n",
    "    def __init__(self, samples, class_to_idx,\n",
    "                 white_noise_path=None, is_training=True,\n",
    "                 max_length=MAX_LENGTH, noise_prob=0.0, snr_range=(5, 20),\n",
    "                 sr=SR, n_mfcc=N_MFCC,\n",
    "                 n_fft=400, hop_length=160, win_length=400, n_mels=None):\n",
    "        self.samples = samples\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.white_noise_path = white_noise_path\n",
    "        self.is_training = is_training\n",
    "        self.max_length = max_length\n",
    "        self.noise_prob = float(noise_prob)\n",
    "        self.snr_range = snr_range\n",
    "        self.sr = sr\n",
    "\n",
    "        if n_mels is None:\n",
    "            n_mels = n_mfcc\n",
    "\n",
    "        self.mfcc = torchaudio.transforms.MFCC(\n",
    "            sample_rate=sr, n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "                \"n_fft\": n_fft,\n",
    "                \"win_length\": win_length,\n",
    "                \"hop_length\": hop_length,\n",
    "                \"n_mels\": n_mels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if not COMMIT_MODE:\n",
    "            if is_training and white_noise_path:\n",
    "                print(f\"[Train] white noise={os.path.basename(white_noise_path)} | p={self.noise_prob} | SNR={snr_range}\")\n",
    "            elif not is_training:\n",
    "                print(\"[Val/Test] NO noise\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path, cname = self.samples[idx]\n",
    "        y = int(self.class_to_idx[cname])\n",
    "\n",
    "        x, sr = torchaudio.load(path)  \n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.mean(dim=0, keepdim=True)  \n",
    "\n",
    "        if sr != self.sr:\n",
    "            x = torchaudio.functional.resample(x, sr, self.sr)\n",
    "\n",
    "        T = x.shape[1]\n",
    "        if T < self.max_length:\n",
    "            x = F.pad(x, (0, self.max_length - T))\n",
    "        else:\n",
    "            x = x[:, :self.max_length]\n",
    "\n",
    "        if self.is_training and self.white_noise_path and (random.random() < self.noise_prob):\n",
    "            snr_db = random.uniform(*self.snr_range)\n",
    "            x = add_specific_noise(x, self.white_noise_path, snr_db, self.sr)\n",
    "\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        feat = self.mfcc(x).squeeze(0).transpose(0, 1).contiguous()\n",
    "\n",
    "        return feat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2502e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:21:48.011336Z",
     "iopub.status.busy": "2026-01-07T00:21:48.010899Z",
     "iopub.status.idle": "2026-01-07T00:22:36.261598Z",
     "shell.execute_reply": "2026-01-07T00:22:36.260769Z"
    },
    "papermill": {
     "duration": 48.257122,
     "end_time": "2026-01-07T00:22:36.264957",
     "exception": false,
     "start_time": "2026-01-07T00:21:48.007835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def collect_samples_4classes(root_dir: str, selected_classes, exts=(\".wav\", \".WAV\")):\n",
    "    samples = []\n",
    "    root = Path(root_dir)\n",
    "\n",
    "    for cname in selected_classes:\n",
    "        cdir = root / cname\n",
    "        if not cdir.is_dir():\n",
    "            if not COMMIT_MODE:\n",
    "                print(f\"Folder kelas tidak ada: {cdir}\")\n",
    "            continue\n",
    "\n",
    "        for ent in cdir.iterdir():\n",
    "            if ent.is_file() and ent.suffix.lower() == \".wav\":\n",
    "                samples.append((str(ent), cname))\n",
    "\n",
    "    samples.sort(key=lambda x: x[0])\n",
    "    return samples\n",
    "\n",
    "all_samples = collect_samples_4classes(ROOT_DIR, SELECTED_CLASSES)\n",
    "\n",
    "if not COMMIT_MODE:\n",
    "    print(f\"Total files (classes={len(SELECTED_CLASSES)}): {len(all_samples)}\")\n",
    "    per_class = Counter([c for _, c in all_samples])\n",
    "    print(\"Per-class counts:\", dict(per_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03d051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:22:36.271169Z",
     "iopub.status.busy": "2026-01-07T00:22:36.270958Z",
     "iopub.status.idle": "2026-01-07T00:22:36.282537Z",
     "shell.execute_reply": "2026-01-07T00:22:36.281817Z"
    },
    "papermill": {
     "duration": 0.016071,
     "end_time": "2026-01-07T00:22:36.283542",
     "exception": false,
     "start_time": "2026-01-07T00:22:36.267471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    ACTIVE_NOISE_PATH\n",
    "except NameError:\n",
    "    ACTIVE_NOISE_PATH = WHITE_PATH\n",
    "    ACTIVE_NOISE_NAME = \"white\"\n",
    "\n",
    "\n",
    "if not COMMIT_MODE:\n",
    "    print(\"[MFCC] Will compute MFCC params per SR using rules function during loader build\")\n",
    "\n",
    "def build_kfold_loaders_generic(\n",
    "    samples, class_to_idx,\n",
    "    sr=None, n_splits=5, seed=42,\n",
    "    batch_size=64, num_workers=2, pin_memory=True,\n",
    "    noise_path=None,\n",
    "    max_length=None,\n",
    "    noise_prob=0.0, snr_range=(5,20),\n",
    "    augment_mode=\"file_noise\",       \n",
    "    norm_mode=\"none\",                \n",
    "    crop_mode=\"left\"\n",
    "):\n",
    "\n",
    "    sr = int(sr if sr is not None else (SR_LIST[-1] if 'SR_LIST' in globals() else 16000))\n",
    "    max_length = int(max_length if max_length is not None else sr)\n",
    "\n",
    "    _mfcc_cfg = mfcc_params_for_sr(sr)\n",
    "    WIN_LENGTH = _mfcc_cfg[\"win_length\"]\n",
    "    HOP_LENGTH = _mfcc_cfg[\"hop_length\"]\n",
    "    N_FFT      = _mfcc_cfg[\"n_fft\"]\n",
    "    if not COMMIT_MODE:\n",
    "        print(f\"[MFCC] SR={sr} -> n_fft={N_FFT} win_length={WIN_LENGTH} hop_length={HOP_LENGTH}\")\n",
    "\n",
    "    y_all = np.array([class_to_idx[c] for _, c in samples], dtype=np.int64)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=int(seed))\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    pm = bool(pin_memory and use_cuda)\n",
    "\n",
    "    dl_args = dict(\n",
    "        batch_size=int(batch_size),\n",
    "        num_workers=int(num_workers),\n",
    "        pin_memory=pm,\n",
    "        persistent_workers=True if int(num_workers) > 0 else False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    if augment_mode == \"file_noise\":\n",
    "        if noise_path is None:\n",
    "            raise ValueError(\"augment_mode='file_noise' membutuhkan noise_path yang valid\")\n",
    "        if not os.path.exists(noise_path):\n",
    "            raise FileNotFoundError(f\"noise_path invalid: {noise_path}\")\n",
    "            \n",
    "    g_base = int(seed) * 1_000_003\n",
    "    folds = []\n",
    "    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(samples, y_all), start=1):\n",
    "        tr_s = [samples[i] for i in tr_idx]\n",
    "        va_s = [samples[i] for i in va_idx]\n",
    "\n",
    "        ds_tr = VoiceCommandDatasetWithNoise(tr_s, class_to_idx,\n",
    "            white_noise_path=noise_path if augment_mode == \"file_noise\" else None,\n",
    "            is_training=True,\n",
    "            sr=sr, max_length=max_length,\n",
    "            n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, n_mels=N_MELS, noise_prob=noise_prob, snr_range=snr_range, \n",
    "        )\n",
    "        ds_va = VoiceCommandDatasetWithNoise(va_s, class_to_idx,\n",
    "            white_noise_path=None, is_training=False,               \n",
    "            sr=sr, max_length=max_length,\n",
    "            n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, n_mels=N_MELS, noise_prob=0.0, snr_range=snr_range, \n",
    "        )\n",
    "\n",
    "        g = torch.Generator(device=\"cpu\"); g.manual_seed(g_base + fold_id)\n",
    "        dl_tr = DataLoader(ds_tr, shuffle=True,  generator=g, **dl_args)\n",
    "        dl_va = DataLoader(ds_va, shuffle=False,                 **dl_args)\n",
    "\n",
    "        if not COMMIT_MODE:\n",
    "            print(f\"[SR={sr} | seed={seed} | fold={fold_id}] \"\n",
    "                  f\"train={len(tr_s)} val={len(va_s)} noise={ACTIVE_NOISE_NAME} (workers={num_workers}, pin_memory={pm})\")\n",
    "\n",
    "        folds.append({\"fold\": fold_id, \"train_loader\": dl_tr, \"val_loader\": dl_va})\n",
    "\n",
    "    return folds\n",
    "\n",
    "def build_kfold_loaders_noise(\n",
    "    samples, class_to_idx, n_splits=5, seed=42,\n",
    "    batch_size=64, num_workers=2, pin_memory=True,\n",
    "    noise_path=None, max_length=None,\n",
    "    noise_prob=NOISE_PROB, snr_range=SNR_RANGE, sr=None\n",
    "):\n",
    "    return build_kfold_loaders_generic(\n",
    "        samples=samples, class_to_idx=class_to_idx,\n",
    "        sr=sr, n_splits=n_splits, seed=seed,\n",
    "        batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory,\n",
    "        noise_path=(noise_path if noise_path is not None else ACTIVE_NOISE_PATH),\n",
    "        max_length=max_length,\n",
    "        noise_prob=noise_prob, snr_range=snr_range,\n",
    "        augment_mode=\"file_noise\",\n",
    "        norm_mode=\"none\", crop_mode=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067d304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:22:36.289342Z",
     "iopub.status.busy": "2026-01-07T00:22:36.289136Z",
     "iopub.status.idle": "2026-01-07T00:22:36.295531Z",
     "shell.execute_reply": "2026-01-07T00:22:36.294983Z"
    },
    "papermill": {
     "duration": 0.010479,
     "end_time": "2026-01-07T00:22:36.296483",
     "exception": false,
     "start_time": "2026-01-07T00:22:36.286004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MFCC_LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mfcc: int = N_MFCC,\n",
    "        hidden: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        num_classes: int = 4,\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        lstm_dropout = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_mfcc,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=lstm_dropout,\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(out_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_dim, num_classes),\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "        lin = self.head[-1]\n",
    "        nn.init.xavier_uniform_(lin.weight)\n",
    "        nn.init.zeros_(lin.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.is_cuda:\n",
    "            self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]              \n",
    "        logits = self.head(out)          \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124397c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:22:36.302364Z",
     "iopub.status.busy": "2026-01-07T00:22:36.302164Z",
     "iopub.status.idle": "2026-01-07T00:22:36.310769Z",
     "shell.execute_reply": "2026-01-07T00:22:36.310229Z"
    },
    "papermill": {
     "duration": 0.012778,
     "end_time": "2026-01-07T00:22:36.311716",
     "exception": false,
     "start_time": "2026-01-07T00:22:36.298938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, scheduler=None, grad_clip_norm=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    use_amp = (\"SCALER\" in globals()) and (SCALER is not None) and (device.type == \"cuda\")\n",
    "    amp_ctx = torch.autocast(device_type=\"cuda\") if use_amp else nullcontext()\n",
    "\n",
    "    step_per_batch = (\n",
    "        scheduler is not None\n",
    "        and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "    )\n",
    "\n",
    "    for feats, y in loader:\n",
    "        feats = feats.to(device, non_blocking=NON_BLOCK).float()\n",
    "        y     = y.to(device, non_blocking=NON_BLOCK).long()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with amp_ctx:\n",
    "            logits = model(feats)\n",
    "            loss   = criterion(logits, y)\n",
    "\n",
    "        if use_amp:\n",
    "            SCALER.scale(loss).backward()\n",
    "            if grad_clip_norm is not None:\n",
    "                SCALER.unscale_(optimizer)\n",
    "                clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n",
    "            SCALER.step(optimizer)\n",
    "            SCALER.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if grad_clip_norm is not None:\n",
    "                clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n",
    "            optimizer.step()\n",
    "\n",
    "        if step_per_batch:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "\n",
    "    return float(total_loss) / float(len(loader.dataset))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    total_loss = 0.0\n",
    "    have_loss = criterion is not None\n",
    "\n",
    "    for feats, y in loader:\n",
    "        feats = feats.to(device, non_blocking=NON_BLOCK).float()\n",
    "        y     = y.to(device, non_blocking=NON_BLOCK).long()\n",
    "\n",
    "        logits = model(feats)\n",
    "        if have_loss:\n",
    "            total_loss += criterion(logits, y).item() * y.size(0)\n",
    "\n",
    "        pred = logits.argmax(dim=1)\n",
    "        all_true.extend(y.tolist())\n",
    "        all_pred.extend(pred.tolist())\n",
    "\n",
    "    acc = accuracy_score(all_true, all_pred)\n",
    "    f1  = f1_score(all_true, all_pred, average=\"macro\")\n",
    "    avg_loss = (float(total_loss) / float(len(loader.dataset))) if have_loss else None\n",
    "\n",
    "    return float(acc), float(f1), avg_loss, np.array(all_true, dtype=np.int64), np.array(all_pred, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0697d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:22:36.317414Z",
     "iopub.status.busy": "2026-01-07T00:22:36.317214Z",
     "iopub.status.idle": "2026-01-07T00:22:37.689256Z",
     "shell.execute_reply": "2026-01-07T00:22:37.688609Z"
    },
    "papermill": {
     "duration": 1.37652,
     "end_time": "2026-01-07T00:22:37.690657",
     "exception": false,
     "start_time": "2026-01-07T00:22:36.314137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMIT_MODE = True\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "\n",
    "    if COMMIT_MODE:\n",
    "        kwargs[\"disable\"] = True\n",
    "    return _tqdm(*args, **kwargs)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "if COMMIT_MODE:\n",
    "    def _no_show(*args, **kwargs):\n",
    "        pass\n",
    "    plt.show = _no_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293a53e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T00:22:37.697820Z",
     "iopub.status.busy": "2026-01-07T00:22:37.697520Z",
     "iopub.status.idle": "2026-01-07T03:08:25.569453Z",
     "shell.execute_reply": "2026-01-07T03:08:25.568465Z"
    },
    "papermill": {
     "duration": 9947.87782,
     "end_time": "2026-01-07T03:08:25.571339",
     "exception": false,
     "start_time": "2026-01-07T00:22:37.693519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io, sys, contextlib, os, time, numpy as np, pandas as pd, shutil, json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SR_LIST = sorted(set(int(s) for s in SR_LIST))\n",
    "\n",
    "\n",
    "def _outdirs_for(SR: int):\n",
    "    root = Path(f\"/kaggle/working/kfold_outputs_sr{SR}\")\n",
    "    d_fold = root / \"per_seed_and_fold\"   \n",
    "    d_sum  = root / \"summary\"             \n",
    "    d_fold.mkdir(parents=True, exist_ok=True)\n",
    "    d_sum.mkdir(parents=True, exist_ok=True)\n",
    "    return root, d_fold, d_sum\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def mute_outputs(active: bool):\n",
    "    if not active:\n",
    "        yield\n",
    "    else:\n",
    "        buf = io.StringIO()\n",
    "        with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "            yield\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "USE_CUDA   = torch.cuda.is_available()\n",
    "device     = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "PIN_MEMORY = bool(USE_CUDA)\n",
    "NON_BLOCK  = bool(USE_CUDA)\n",
    "if not COMMIT_MODE:\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "AMP_ENABLE = USE_CUDA\n",
    "if AMP_ENABLE:\n",
    "    major_cc = torch.cuda.get_device_capability()[0]\n",
    "    AMP_DTYPE = torch.bfloat16 if major_cc >= 8 else torch.float16\n",
    "    try:\n",
    "        SCALER = torch.amp.GradScaler(device=\"cuda\") if AMP_DTYPE is torch.float16 else None\n",
    "    except Exception:\n",
    "        SCALER = torch.cuda.amp.GradScaler(enabled=(AMP_DTYPE is torch.float16))\n",
    "else:\n",
    "    AMP_DTYPE = None\n",
    "    SCALER = None\n",
    "\n",
    "results = []\n",
    "summary_rows = []\n",
    "efficiency_rows_global = []\n",
    "\n",
    "with mute_outputs(COMMIT_MODE):\n",
    "    for seed in SEEDS:\n",
    "        if not COMMIT_MODE:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"Running SEED = {seed}\")\n",
    "            print(\"=\"*70)\n",
    "        set_seed(seed)\n",
    "\n",
    "        for sr in SR_LIST:\n",
    "            SR = int(sr)\n",
    "            ROOT_OUT, OUT_FOLD, OUT_SUMM = _outdirs_for(SR)\n",
    "            MAX_LENGTH = SR\n",
    "\n",
    "            folds = build_kfold_loaders_noise(\n",
    "                all_samples, CLASS_TO_IDX,\n",
    "                n_splits=N_SPLITS, seed=seed,\n",
    "                batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                noise_path=ACTIVE_NOISE_PATH,\n",
    "                max_length=MAX_LENGTH, noise_prob=NOISE_PROB, snr_range=SNR_RANGE,\n",
    "                sr=SR\n",
    "            )\n",
    "\n",
    "            for fd in folds:\n",
    "                fold_id     = fd[\"fold\"]\n",
    "                train_loader= fd[\"train_loader\"]\n",
    "                val_loader  = fd[\"val_loader\"]\n",
    "\n",
    "                RUN_NAME = f\"lstm_seed{seed}_fold{fold_id}_sr{SR}\"\n",
    "                OUT_SUB  = str((ROOT_OUT / \"per_seed_and_fold\" / RUN_NAME).resolve())\n",
    "                os.makedirs(OUT_SUB, exist_ok=True)\n",
    "                BEST_CKPT = os.path.join(OUT_SUB, \"best_model.pth\")\n",
    "                BEST_FULL = os.path.join(OUT_SUB, \"best_full.pt\")\n",
    "                LAST_CKPT = os.path.join(OUT_SUB, \"last_model.pth\")\n",
    "                \n",
    "                model = MFCC_LSTM(n_mfcc=N_MFCC, num_classes=len(SELECTED_CLASSES)).to(device)\n",
    "                assert next(model.parameters()).is_cuda == USE_CUDA, \"Model belum di CUDA!\"\n",
    "                n_params = int(sum(p.numel() for p in model.parameters()))\n",
    "                \n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "                total_steps = max(1, EPOCHS * len(train_loader))\n",
    "                scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                    optimizer, max_lr=1e-3, total_steps=total_steps\n",
    "                )\n",
    "                criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "                best_val_acc = -1.0\n",
    "                best_true = best_pred = None\n",
    "                current_best_ckpt_path = BEST_CKPT\n",
    "                history = []  # {epoch, train_loss, val_loss, val_acc, val_f1, epoch_time_sec, throughput_samples_per_sec}\n",
    "\n",
    "                for ep in range(1, EPOCHS+1):\n",
    "                    model.train()\n",
    "                    tr_loss_sum = 0.0\n",
    "                    n_train = 0\n",
    "                    ep_t0 = time.time()\n",
    "\n",
    "                    for feats, y in train_loader:\n",
    "                        feats = feats.to(device, non_blocking=NON_BLOCK).float()\n",
    "                        y     = y.to(device, non_blocking=NON_BLOCK).long()\n",
    "\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                        if AMP_ENABLE:\n",
    "                            with torch.autocast(device_type='cuda', dtype=AMP_DTYPE):\n",
    "                                logits = model(feats)\n",
    "                                loss = criterion(logits, y)\n",
    "                            if SCALER is not None:\n",
    "                                SCALER.scale(loss).backward()\n",
    "                                SCALER.step(optimizer)\n",
    "                                SCALER.update()\n",
    "                            else:\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                        else:\n",
    "                            logits = model(feats)\n",
    "                            loss = criterion(logits, y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "\n",
    "                        bs = y.size(0)\n",
    "                        tr_loss_sum += loss.item() * bs\n",
    "                        n_train += bs\n",
    "\n",
    "                    tr_loss = tr_loss_sum / max(1, n_train)\n",
    "\n",
    "                    val_acc, val_f1, val_loss, y_true, y_pred = evaluate(model, val_loader, criterion)\n",
    "\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = float(val_acc)\n",
    "                        best_true = y_true.copy()\n",
    "                        best_pred = y_pred.copy()\n",
    "\n",
    "                        torch.save(model.state_dict(), BEST_CKPT)\n",
    "                        torch.save({\n",
    "                            \"epoch\": ep,\n",
    "                            \"model_state\": model.state_dict(),\n",
    "                            \"optimizer_state\": optimizer.state_dict(),\n",
    "                            \"scheduler_state\": scheduler.state_dict() if scheduler is not None else None,\n",
    "                            \"val_acc\": best_val_acc,\n",
    "                            \"sr\": SR,\n",
    "                            \"seed\": seed,\n",
    "                            \"fold\": fold_id,\n",
    "                            \"classes\": list(SELECTED_CLASSES),\n",
    "                            \"class_to_idx\": CLASS_TO_IDX,\n",
    "                            \"n_mfcc\": N_MFCC,\n",
    "                        }, BEST_FULL)\n",
    "                        current_best_ckpt_path = BEST_CKPT\n",
    "\n",
    "                    ep_time = time.time() - ep_t0\n",
    "                    thr = float(n_train) / ep_time if ep_time > 0 else 0.0\n",
    "                    history.append({\n",
    "                        \"epoch\": ep,\n",
    "                        \"train_loss\": float(tr_loss),\n",
    "                        \"val_loss\": float(val_loss),\n",
    "                        \"val_acc\": float(val_acc),\n",
    "                        \"val_f1\": float(val_f1),\n",
    "                        \"epoch_time_sec\": float(ep_time),\n",
    "                        \"throughput_samples_per_sec\": float(thr),\n",
    "                    })\n",
    "\n",
    "                    if (ep % 5 == 0 or ep == 1 or ep == EPOCHS) and not COMMIT_MODE:\n",
    "                        if USE_CUDA:\n",
    "                            gpu_mb = torch.cuda.memory_allocated() / 1e6\n",
    "                            print(f\"[SR={SR} | GPU {gpu_mb:.1f} MB]\", end=\" \")\n",
    "                        print(f\"Seed {seed} | Fold {fold_id} | Epoch {ep:02d} \"\n",
    "                              f\"| tr_loss={tr_loss:.4f} | va_loss={val_loss:.4f} \"\n",
    "                              f\"| va_acc={val_acc:.4f} | va_f1={val_f1:.4f} | ep_time={ep_time:.2f}s | thr={thr:.1f}/s\")\n",
    "\n",
    "                torch.save(model.state_dict(), LAST_CKPT)\n",
    "\n",
    "                # Confusion matrix\n",
    "                cm = confusion_matrix(best_true, best_pred, labels=list(range(len(SELECTED_CLASSES))))\n",
    "                cm_df = pd.DataFrame(cm, index=SELECTED_CLASSES, columns=SELECTED_CLASSES)\n",
    "                cm_path = os.path.join(OUT_FOLD, f\"cm_lstm_seed{seed}_fold{fold_id}_sr{SR}.csv\")\n",
    "                cm_df.to_csv(cm_path)\n",
    "\n",
    "                # Save history CSV\n",
    "                hist_df = pd.DataFrame(history)\n",
    "                hist_csv = os.path.join(OUT_SUB, \"history.csv\")\n",
    "                hist_df.to_csv(hist_csv, index=False)\n",
    "\n",
    "                # Plots: loss and metrics\n",
    "                try:\n",
    "                    # Loss curves\n",
    "                    plt.figure(figsize=(8,5))\n",
    "                    plt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"train_loss\")\n",
    "                    plt.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"], label=\"val_loss\")\n",
    "                    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curve\"); plt.legend()\n",
    "                    plt.tight_layout(); plt.savefig(os.path.join(OUT_SUB, \"loss_curve.png\")); plt.close()\n",
    "\n",
    "                    # Metric curves\n",
    "                    plt.figure(figsize=(8,5))\n",
    "                    plt.plot(hist_df[\"epoch\"], hist_df[\"val_acc\"], label=\"val_acc\")\n",
    "                    plt.plot(hist_df[\"epoch\"], hist_df[\"val_f1\"], label=\"val_f1\")\n",
    "                    plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(\"Validation Metrics\"); plt.legend()\n",
    "                    plt.tight_layout(); plt.savefig(os.path.join(OUT_SUB, \"metrics_curve.png\")); plt.close()\n",
    "\n",
    "                    # Error per class bar (1 - recall)\n",
    "                    per_class_counts = cm_df.sum(axis=1).replace(0, np.nan)\n",
    "                    correct = np.diag(cm)\n",
    "                    recall = correct / per_class_counts.values\n",
    "                    err_rate = 1.0 - recall\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.bar(cm_df.index, err_rate)\n",
    "                    plt.ylabel(\"Error rate (1 - recall)\"); plt.title(\"Error per Class\")\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                    plt.tight_layout(); plt.savefig(os.path.join(OUT_SUB, \"error_per_class.png\")); plt.close()\n",
    "\n",
    "                    # Confusion matrix heatmap\n",
    "                    plt.figure(figsize=(6,5))\n",
    "                    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "                    plt.title('Confusion Matrix'); plt.colorbar()\n",
    "                    tick_marks = np.arange(len(SELECTED_CLASSES))\n",
    "                    plt.xticks(tick_marks, SELECTED_CLASSES, rotation=45, ha='right')\n",
    "                    plt.yticks(tick_marks, SELECTED_CLASSES)\n",
    "                    plt.tight_layout(); plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "                    plt.savefig(os.path.join(OUT_SUB, \"confusion_matrix.png\")); plt.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Failed to plot curves/CM for seed={seed} fold={fold_id} SR={SR}: {e}\")\n",
    "\n",
    "                # ROC & PR curves using best checkpoint\n",
    "                try:\n",
    "                    from sklearn.preprocessing import label_binarize\n",
    "                    from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "                    # Load best weights \n",
    "                    try:\n",
    "                        state = torch.load(current_best_ckpt_path, map_location=device)\n",
    "                        if isinstance(state, dict) and not any(k.startswith('layer') for k in state.keys()) and 'model_state' in state:\n",
    "                            model.load_state_dict(state['model_state'])\n",
    "                        else:\n",
    "                            model.load_state_dict(state)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    model.eval()\n",
    "\n",
    "                    y_true_list, y_score_chunks = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for feats, y in val_loader:\n",
    "                            feats = feats.to(device, non_blocking=NON_BLOCK).float()\n",
    "                            logits = model(feats)\n",
    "                            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "                            y_score_chunks.append(probs)\n",
    "                            y_true_list.extend(y.numpy().tolist())\n",
    "\n",
    "                    y_true_np = np.array(y_true_list, dtype=np.int64)\n",
    "                    y_score_np = np.vstack(y_score_chunks) if y_score_chunks else np.zeros((0, len(SELECTED_CLASSES)))\n",
    "                    n_classes = len(SELECTED_CLASSES)\n",
    "                    if y_score_np.shape[0] > 0:\n",
    "                        y_bin = label_binarize(y_true_np, classes=list(range(n_classes)))\n",
    "                        # ROC curves\n",
    "                        fpr, tpr, roc_auc = {}, {}, {}\n",
    "                        for i in range(n_classes):\n",
    "                            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score_np[:, i])\n",
    "                            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "                        fpr['micro'], tpr['micro'], _ = roc_curve(y_bin.ravel(), y_score_np.ravel())\n",
    "                        roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "                        roc_auc_macro = roc_auc_score(y_bin, y_score_np, average='macro', multi_class='ovr')\n",
    "\n",
    "                        plt.figure(figsize=(8,6))\n",
    "                        for i, name in enumerate(SELECTED_CLASSES):\n",
    "                            plt.plot(fpr[i], tpr[i], label=f\"{name} (AUC={roc_auc[i]:.3f})\")\n",
    "                        plt.plot([0,1],[0,1], 'k--', alpha=0.4)\n",
    "                        plt.plot(fpr['micro'], tpr['micro'], linestyle='--', label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "                        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(\"ROC Curves (OvR)\")\n",
    "                        plt.legend()\n",
    "                        plt.tight_layout(); plt.savefig(os.path.join(OUT_SUB, \"roc_curve.png\")); plt.close()\n",
    "\n",
    "                        auc_rows = ([{'class': SELECTED_CLASSES[i], 'roc_auc': float(roc_auc[i])} for i in range(n_classes)] +\n",
    "                                    [{'class': 'micro', 'roc_auc': float(roc_auc['micro'])},\n",
    "                                     {'class': 'macro', 'roc_auc': float(roc_auc_macro)}])\n",
    "                        pd.DataFrame(auc_rows).to_csv(os.path.join(OUT_SUB, \"roc_auc_summary.csv\"), index=False)\n",
    "\n",
    "                        # PR curves\n",
    "                        precision, recall, ap = {}, {}, {}\n",
    "                        for i in range(n_classes):\n",
    "                            precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], y_score_np[:, i])\n",
    "                            ap[i] = average_precision_score(y_bin[:, i], y_score_np[:, i])\n",
    "                        precision['micro'], recall['micro'], _ = precision_recall_curve(y_bin.ravel(), y_score_np.ravel())\n",
    "                        ap_micro = average_precision_score(y_bin, y_score_np, average='micro')\n",
    "                        ap_macro = average_precision_score(y_bin, y_score_np, average='macro')\n",
    "\n",
    "                        plt.figure(figsize=(8,6))\n",
    "                        for i, name in enumerate(SELECTED_CLASSES):\n",
    "                            plt.plot(recall[i], precision[i], label=f\"{name} (AP={ap[i]:.3f})\")\n",
    "                        plt.plot(recall['micro'], precision['micro'], linestyle='--', label=f\"micro (AP={ap_micro:.3f})\")\n",
    "                        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall Curves (OvR)\")\n",
    "                        plt.legend()\n",
    "                        plt.tight_layout(); plt.savefig(os.path.join(OUT_SUB, \"pr_curve.png\")); plt.close()\n",
    "\n",
    "                        ap_rows = ([{'class': SELECTED_CLASSES[i], 'average_precision': float(ap[i])} for i in range(n_classes)] +\n",
    "                                   [{'class': 'micro', 'average_precision': float(ap_micro)},\n",
    "                                    {'class': 'macro', 'average_precision': float(ap_macro)}])\n",
    "                        pd.DataFrame(ap_rows).to_csv(os.path.join(OUT_SUB, \"pr_ap_summary.csv\"), index=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Failed to compute PR/ROC curves for seed={seed} fold={fold_id} SR={SR}: {e}\")\n",
    "\n",
    "                # Efficiency rows per-fold\n",
    "                total_time = float(hist_df[\"epoch_time_sec\"].sum()) if not hist_df.empty else 0.0\n",
    "                avg_ep_time = float(hist_df[\"epoch_time_sec\"].mean()) if not hist_df.empty else 0.0\n",
    "                mean_thr = float(hist_df[\"throughput_samples_per_sec\"].mean()) if not hist_df.empty else 0.0\n",
    "                efficiency_rows_global.append({\n",
    "                    \"sr\": SR,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_id,\n",
    "                    \"arch\": \"LSTM\",\n",
    "                    \"n_params\": n_params,\n",
    "                    \"epochs\": EPOCHS,\n",
    "                    \"total_time_sec\": total_time,\n",
    "                    \"avg_epoch_time_sec\": avg_ep_time,\n",
    "                    \"mean_throughput_samples_per_sec\": mean_thr,\n",
    "                })\n",
    "\n",
    "                results.append({\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_id,\n",
    "                    \"sr\": int(SR),\n",
    "                    \"val_acc\": float(best_val_acc),\n",
    "                    \"cm_path\": cm_path,\n",
    "                    \"ckpt_path\": current_best_ckpt_path,\n",
    "                })\n",
    "\n",
    "        for sr0 in SR_LIST:\n",
    "            accs = [r[\"val_acc\"] for r in results if r[\"seed\"] == seed and r[\"sr\"] == int(sr0)]\n",
    "            if accs:\n",
    "                summary_rows.append({\n",
    "                    \"seed\": seed,\n",
    "                    \"sr\": int(sr0),\n",
    "                    \"acc_mean_over_folds\": float(np.mean(accs)),\n",
    "                    \"acc_std_over_folds\":  float(np.std(accs)),\n",
    "                    \"n_folds\": N_SPLITS\n",
    "                })\n",
    "\n",
    "# Efficiency summaries per SR\n",
    "if efficiency_rows_global:\n",
    "    eff_df = pd.DataFrame(efficiency_rows_global)\n",
    "    for SR in sorted(set(eff_df[\"sr\"].tolist())):\n",
    "        ROOT_OUT, OUT_FOLD, OUT_SUMM = _outdirs_for(SR)\n",
    "        eff_df_sr = eff_df[eff_df[\"sr\"] == int(SR)].copy()\n",
    "        if not eff_df_sr.empty:\n",
    "            eff_df_sr.to_csv(OUT_SUMM / f\"lstm_speed_efficiency_per_fold_sr{SR}.csv\", index=False)\n",
    "            agg = (\n",
    "                eff_df_sr.groupby([\"seed\"]).agg({\n",
    "                    \"total_time_sec\": \"sum\",\n",
    "                    \"avg_epoch_time_sec\": \"mean\",\n",
    "                    \"mean_throughput_samples_per_sec\": \"mean\",\n",
    "                    \"fold\": \"count\"\n",
    "                }).rename(columns={\"fold\": \"n_folds\"}).reset_index()\n",
    "            )\n",
    "            agg.to_csv(OUT_SUMM / f\"lstm_speed_efficiency_per_seed_sr{SR}.csv\", index=False)\n",
    "\n",
    "if not COMMIT_MODE:\n",
    "    print(\"\\nTraining selesai.\")\n",
    "\n",
    "df_results_all = pd.DataFrame(results)\n",
    "df_seed_all    = pd.DataFrame(summary_rows)\n",
    "\n",
    "for SR in sorted(set(SR_LIST)):\n",
    "    ROOT_OUT, OUT_FOLD, OUT_SUMM = _outdirs_for(SR)\n",
    "\n",
    "    if not df_results_all.empty:\n",
    "        df_res_sr = df_results_all[df_results_all[\"sr\"] == int(SR)].copy()\n",
    "        if len(df_res_sr):\n",
    "            df_res_sr.to_csv(OUT_FOLD / f\"lstm_kfold_results_per_fold_sr{SR}.csv\", index=False)\n",
    "\n",
    "    if not df_seed_all.empty:\n",
    "        df_seed_sr = df_seed_all[df_seed_all[\"sr\"] == int(SR)].copy()\n",
    "        if len(df_seed_sr):\n",
    "            df_seed_sr.to_csv(OUT_SUMM / f\"lstm_kfold_summary_per_seed_sr{SR}.csv\", index=False)\n",
    "\n",
    "            mu_acc = float(df_seed_sr[\"acc_mean_over_folds\"].mean())\n",
    "            sd_acc = float(df_seed_sr[\"acc_mean_over_folds\"].std(ddof=1)) if len(df_seed_sr) > 1 else 0.0\n",
    "\n",
    "            df_sr_summary = pd.DataFrame([{\n",
    "                \"model\": \"LSTM\",\n",
    "                \"sr\": int(SR),\n",
    "                \"acc_mean\": mu_acc,\n",
    "                \"acc_sd\": sd_acc,\n",
    "                \"n_seeds\": int(df_seed_sr[\"seed\"].nunique()),\n",
    "                \"kfold\": int(df_seed_sr[\"n_folds\"].max()) if \"n_folds\" in df_seed_sr else 5\n",
    "            }])\n",
    "            df_sr_summary.to_csv(OUT_SUMM / f\"lstm_kfold_multi_seed_summary_sr{SR}.csv\", index=False)\n",
    "\n",
    "    df_res_sr = df_results_all[df_results_all[\"sr\"] == int(SR)].copy()\n",
    "    if not df_res_sr.empty:\n",
    "        manifest_path = OUT_SUMM / f\"lstm_best_checkpoints_per_seed_fold_sr{SR}.csv\"\n",
    "        cols = [\"seed\", \"fold\", \"sr\", \"val_acc\", \"ckpt_path\", \"cm_path\"]\n",
    "        (df_res_sr[cols].sort_values([\"seed\", \"fold\"])\n",
    "         .to_csv(manifest_path, index=False))\n",
    "\n",
    "        idx_per_seed = df_res_sr.groupby(\"seed\")[\"val_acc\"].idxmax()\n",
    "        df_best_per_seed = df_res_sr.loc[idx_per_seed].copy().sort_values([\"seed\"]).reset_index(drop=True)\n",
    "\n",
    "        per_seed_csv = OUT_SUMM / f\"lstm_best_per_seed_sr{SR}.csv\"\n",
    "        df_best_per_seed[[\"seed\", \"fold\", \"sr\", \"val_acc\", \"ckpt_path\"]].to_csv(per_seed_csv, index=False)\n",
    "\n",
    "        for _, row in df_best_per_seed.iterrows():\n",
    "            src = Path(row[\"ckpt_path\"])\n",
    "            dst = OUT_SUMM / f\"bestmodel_seed{int(row['seed'])}_sr{int(SR)}.pth\"\n",
    "            try:\n",
    "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(src, dst)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN][SR={SR}] Gagal salin best per seed {int(row['seed'])}: {e}\")\n",
    "\n",
    "        row_sr_best = df_res_sr.loc[df_res_sr[\"val_acc\"].idxmax()]\n",
    "        src = Path(row_sr_best[\"ckpt_path\"])\n",
    "        dst = ROOT_OUT / f\"bestmodel_sr{int(SR)}.pth\"\n",
    "        try:\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN][SR={SR}] Gagal salin best per SR: {e}\")\n",
    "\n",
    "if not df_results_all.empty:\n",
    "    row_global = df_results_all.loc[df_results_all[\"val_acc\"].idxmax()]\n",
    "    src = Path(row_global[\"ckpt_path\"])\n",
    "    final_dst = Path(\"/kaggle/working\") / \"bestmodel.pth\"\n",
    "    try:\n",
    "        final_dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(src, final_dst)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Gagal salin global best: {e}\")\n",
    "\n",
    "legacy_dir = \"/kaggle/working/kfold_outputs\"\n",
    "os.makedirs(legacy_dir, exist_ok=True)\n",
    "\n",
    "all_best_rows = [] \n",
    "\n",
    "for SR in SR_LIST:\n",
    "    ROOT_OUT, OUT_FOLD, OUT_SUMM = _outdirs_for(SR)\n",
    "    df_res_sr = df_results_all[df_results_all[\"sr\"] == int(SR)].copy()\n",
    "    if df_res_sr.empty:\n",
    "        continue\n",
    "\n",
    "    idx_per_seed = df_res_sr.groupby(\"seed\")[\"val_acc\"].idxmax()\n",
    "    df_best_per_seed = df_res_sr.loc[idx_per_seed].copy().sort_values([\"seed\"]).reset_index(drop=True)\n",
    "\n",
    "    best_rows_sr = []\n",
    "    for _, rec in df_best_per_seed.iterrows():\n",
    "        slim = {\n",
    "            \"seed\": int(rec[\"seed\"]),\n",
    "            \"fold\": int(rec[\"fold\"]),\n",
    "            \"sr\":   int(rec[\"sr\"]),\n",
    "            \"val_acc\": float(rec[\"val_acc\"]),\n",
    "            \"cm_path\": rec.get(\"cm_path\", None),\n",
    "            \"ckpt_path\": rec.get(\"ckpt_path\", None),\n",
    "        }\n",
    "        best_rows_sr.append(slim)\n",
    "        with open(OUT_SUMM / f\"best_seed{slim['seed']}_fold{slim['fold']}_sr{SR}.json\", \"w\") as f:\n",
    "            json.dump(slim, f, indent=2)\n",
    "\n",
    "    pd.DataFrame(best_rows_sr).to_csv(OUT_SUMM / f\"best_per_seed_sr{SR}.csv\", index=False)\n",
    "    with open(OUT_SUMM / f\"best_per_seed_sr{SR}.json\", \"w\") as f:\n",
    "        json.dump(best_rows_sr, f, indent=2)\n",
    "\n",
    "    all_best_rows.extend(best_rows_sr)\n",
    "\n",
    "if all_best_rows:\n",
    "    df_best_all = pd.DataFrame(all_best_rows).sort_values([\"sr\", \"seed\"]).reset_index(drop=True)\n",
    "    df_best_path_csv  = os.path.join(legacy_dir, \"best_per_seed.csv\")\n",
    "    df_best_path_json = os.path.join(legacy_dir, \"best_per_seed.json\")\n",
    "    df_best_all.to_csv(df_best_path_csv, index=False)\n",
    "    with open(df_best_path_json, \"w\") as f:\n",
    "        json.dump(all_best_rows, f, indent=2)\n",
    "\n",
    "    idx_best_seed_global = df_results_all.groupby(\"seed\")[\"val_acc\"].idxmax()\n",
    "    for _, rec in df_results_all.loc[idx_best_seed_global].iterrows():\n",
    "        slim = {\n",
    "            \"seed\": int(rec[\"seed\"]),\n",
    "            \"fold\": int(rec[\"fold\"]),\n",
    "            \"sr\":   int(rec[\"sr\"]),\n",
    "            \"val_acc\": float(rec[\"val_acc\"]),\n",
    "            \"cm_path\": rec.get(\"cm_path\", None),\n",
    "            \"ckpt_path\": rec.get(\"ckpt_path\", None),\n",
    "        }\n",
    "        with open(os.path.join(legacy_dir, f\"best_seed{slim['seed']}_fold{slim['fold']}.json\"), \"w\") as f:\n",
    "            json.dump(slim, f, indent=2)\n",
    "\n",
    "if not COMMIT_MODE:\n",
    "    for SR in SR_LIST:\n",
    "        ROOT_OUT, OUT_FOLD, OUT_SUMM = _outdirs_for(SR)\n",
    "        print(f\"\\n[SR={SR}] Saved to:\")\n",
    "        print(f\"  {OUT_FOLD}/lstm_kfold_results_per_fold_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/lstm_kfold_summary_per_seed_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/lstm_kfold_multi_seed_summary_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/lstm_best_checkpoints_per_seed_fold_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/lstm_best_per_seed_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/lstm_speed_efficiency_per_fold_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/lstm_speed_efficiency_per_seed_sr{SR}.csv\")\n",
    "        print(f\"  {OUT_SUMM}/best_per_seed_sr{SR}.json\")\n",
    "        print(f\"  {ROOT_OUT}/bestmodel_sr{SR}.pth\")\n",
    "        print(f\"  /kaggle/working/bestmodel.pth (global, opsional)\")\n",
    "        print(f\"  /kaggle/working/kfold_outputs/best_per_seed.csv|json (legacy)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6666857,
     "sourceId": 10749559,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10008.869628,
   "end_time": "2026-01-07T03:08:28.193162",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-07T00:21:39.323534",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
